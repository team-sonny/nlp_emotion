{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Multi-modal mlp mixer"
      ],
      "metadata": {
        "id": "9lHB1A25AYb0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGNsjjEMr8A_",
        "outputId": "3dbdb23b-824b-44f1-9c8c-be529d3c2271"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3c4GcMSIfMZ",
        "outputId": "8eb33da9-3ac7-4a9d-b06d-a79d7ad97846"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/sonny\n"
          ]
        }
      ],
      "source": [
        "# cd drive/MyDrive/sonny"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwQzik-b7vdl"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install einops"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare dataset"
      ],
      "metadata": {
        "id": "hO4xEOcv-dfB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1Zf4mRmCHnX"
      },
      "outputs": [],
      "source": [
        "import soundfile as sf\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "N_SAMPLES = 480000\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GekMG-Ju9Nvc"
      },
      "outputs": [],
      "source": [
        "def pad_or_trim(array, length: int = N_SAMPLES, *, axis: int = -1):\n",
        "    \"\"\"\n",
        "    Pad or trim the audio array to N_SAMPLES, as expected by the encoder.\n",
        "    \"\"\"\n",
        "    if torch.is_tensor(array):\n",
        "        if array.shape[axis] > length:\n",
        "            array = array.index_select(\n",
        "                dim=axis, index=torch.arange(length, device=array.device)\n",
        "            )\n",
        "\n",
        "        if array.shape[axis] < length:\n",
        "            pad_widths = [(0, 0)] * array.ndim\n",
        "            pad_widths[axis] = (0, length - array.shape[axis])\n",
        "            array = F.pad(array, [pad for sizes in pad_widths[::-1] for pad in sizes])\n",
        "    else:\n",
        "        if array.shape[axis] > length:\n",
        "            array = array.take(indices=range(length), axis=axis)\n",
        "\n",
        "        if array.shape[axis] < length:\n",
        "            pad_widths = [(0, 0)] * array.ndim\n",
        "            pad_widths[axis] = (0, length - array.shape[axis])\n",
        "            array = np.pad(array, pad_widths)\n",
        "\n",
        "    return array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YByqwCOI9VJh"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, csv_file):\n",
        "        self.data = pd.read_csv(csv_file, header=[0, 1])\n",
        "        self.text_data = self.data['text_data'][' '].values\n",
        "        self.wav_dir = self.data['wav_dir'][' '].values\n",
        "        self.dic = {'happy': 0, 'surprise': 1, 'angry': 2, 'neutral': 3, 'disqust': 4, 'fear': 5, 'sad': 6}\n",
        "        self.labels = self.data['Total Evaluation']['Emotion'].values\n",
        "      \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if ';' in self.labels[idx]:\n",
        "          self.labels[idx] = self.labels[idx].split(';')[random.choice([0,1])]\n",
        "\n",
        "        audio_input, sample_rate = sf.read(self.wav_dir[idx])\n",
        "        audio_input = pad_or_trim(audio_input)\n",
        "        audio_input_values = processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\").input_values.squeeze(0)\n",
        "\n",
        "        return self.text_data[idx], audio_input_values, self.dic[self.labels[idx]]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODELING\n",
        "1. MLP MIXER\n",
        "2. concat\n",
        "3. Cross-Attention\n",
        "4. 3-way concat"
      ],
      "metadata": {
        "id": "EdCWAULm-uoI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqeAeZSxfYWy"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from einops import rearrange, reduce, asnumpy, parse_shape\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5pxTHmWaPPF"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel, Wav2Vec2ForCTC"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. MLP MIXER"
      ],
      "metadata": {
        "id": "tpHC1Brk_OGp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVZ1_vEdAysu"
      },
      "outputs": [],
      "source": [
        "pair = lambda x: x if isinstance(x, tuple) else (x, x)\n",
        "\n",
        "# mlp_mixer_block\n",
        "class PreNormResidual(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        #self.rearrange = Rearrange('b c d -> b d c')\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fn(self.norm(x)) + x\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, expansion_factor = 4, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.inner_dim = int(dim * expansion_factor)\n",
        "        self.dropout = dropout\n",
        "        self.dense_1 = nn.Linear(dim, self.inner_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dense_2 = nn.Linear(self.inner_dim, dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.dense_1(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.dropout_1(x)\n",
        "        x = self.dense_2(x)\n",
        "        x = self.dropout_1(x)\n",
        "        return x\n",
        "\n",
        "class MLPMixer(nn.Module):\n",
        "    def __init__(self, dim, num_classes, expansion_factor = 4, expansion_factor_token = 0.5, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.num_classes = num_classes\n",
        "        self.expansion_factor = expansion_factor\n",
        "        self.expansion_factor_token = expansion_factor_token\n",
        "        self.dropout = dropout\n",
        "        self.linear_1 = nn.Linear\n",
        "        self.rearrange = Rearrange('b c d -> b d c')\n",
        "        self.preNormResidual_1 = PreNormResidual(dim, FeedForward(dim, expansion_factor, dropout))\n",
        "        self.preNormResidual_2 = PreNormResidual(dim, FeedForward(dim, expansion_factor_token, dropout))\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.reduce_ = Reduce('b n c -> b c', 'mean')\n",
        "        self.linear_2 = nn.Linear(dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rearrange(x)\n",
        "        x = self.preNormResidual_1(x)\n",
        "        x = self.preNormResidual_2(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.reduce_(x)\n",
        "        #x = self.linear_2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xREuGCdFk0e7",
        "outputId": "e1792c1e-e0e1-4285-aae6-b4f51c9e26f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MLPMixer(\n",
              "  (rearrange): Rearrange('b c d -> b d c')\n",
              "  (preNormResidual_1): PreNormResidual(\n",
              "    (fn): FeedForward(\n",
              "      (dense_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      (gelu): GELU(approximate='none')\n",
              "      (dropout_1): Dropout(p=0.0, inplace=False)\n",
              "      (dense_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "    )\n",
              "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (preNormResidual_2): PreNormResidual(\n",
              "    (fn): FeedForward(\n",
              "      (dense_1): Linear(in_features=768, out_features=384, bias=True)\n",
              "      (gelu): GELU(approximate='none')\n",
              "      (dropout_1): Dropout(p=0.0, inplace=False)\n",
              "      (dense_2): Linear(in_features=384, out_features=768, bias=True)\n",
              "    )\n",
              "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  (reduce_): Reduce('b n c -> b c', 'mean')\n",
              "  (linear_2): Linear(in_features=768, out_features=7, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mlpMixer_model = MLPMixer(dim = 768, num_classes = 7)\n",
        "mlpMixer_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1abi0UU911m"
      },
      "outputs": [],
      "source": [
        "# mlp_mixer_model\n",
        "class Classify(nn.Module):\n",
        "  def __init__(self, input_size, class_num):\n",
        "    super(Classify, self).__init__()\n",
        "    self.fc = nn.Linear(input_size, class_num)\n",
        "\n",
        "  def forward(self, avg_vecs):\n",
        "    logit = self.fc(avg_vecs)\n",
        "    return logit\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.wav_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\", output_hidden_states=True)\n",
        "        self.txt_model = AutoModel.from_pretrained(\"beomi/KcELECTRA-base-v2022\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base-v2022\")\n",
        "        self.wav_encoder\n",
        "        self.txt_encoder\n",
        "        self.project1 = Project(768)\n",
        "        self.project2 = Project(768)\n",
        "        self.classification_model = Classify(768, 7)\n",
        "        self.mlp_mixer_model = MLPMixer(dim = 768, num_classes = 7)\n",
        "      \n",
        "\n",
        "    def wav_encoder(self, wav_dir):\n",
        "        logits = self.wav_model(wav_dir)['hidden_states'][-1]\n",
        "        return logits\n",
        "\n",
        "    def txt_encoder(self, text_tensor):\n",
        "        outputs = self.txt_model(**txt_tensor)\n",
        "        last_hidden_states = outputs.last_hidden_state\n",
        "        return last_hidden_states\n",
        "\n",
        "    def forward(self, text_data, wav_dir):\n",
        "        encoder_layer_1 = self.wav_encoder(wav_dir)\n",
        "        encoder_layer_2 = self.txt_encoder(text_data)\n",
        "\n",
        "        out1 = self.project1(encoder_layer_1)\n",
        "        out2 = self.project2(encoder_layer_2)\n",
        "\n",
        "        concat = torch.cat([out1, out2], dim = 1)\n",
        "\n",
        "        concat = concat.transpose(1,2)\n",
        "        result = self.mlp_mixer_model(concat)\n",
        "\n",
        "        logit = self.classification_model(result).squeeze(1)\n",
        "        softmax = F.softmax(logit, dim=1)\n",
        "        prediction = torch.argmax(softmax, dim=1)\n",
        "\n",
        "        return logit, softmax, prediction\n",
        "\n",
        "class Project(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "      super().__init__()\n",
        "      self.layer = nn.Linear(dim, dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "      return self.layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. concat"
      ],
      "metadata": {
        "id": "Rx5_ADRr_W9R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5_fW8VbZ1-8"
      },
      "outputs": [],
      "source": [
        "# concat\n",
        "class Classify(nn.Module):\n",
        "  def __init__(self, input_size, class_num):\n",
        "    super(Classify, self).__init__()\n",
        "    self.fc = nn.Linear(input_size, class_num)\n",
        "\n",
        "  def forward(self, avg_vecs):\n",
        "    logit = self.fc(avg_vecs)\n",
        "    return logit\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.wav_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\", output_hidden_states=True)\n",
        "        self.txt_model = AutoModel.from_pretrained(\"beomi/KcELECTRA-base-v2022\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base-v2022\")\n",
        "        self.wav_encoder\n",
        "        self.txt_encoder\n",
        "        self.project1 = Project(768)\n",
        "        self.project2 = Project(768)\n",
        "        self.classification_model = Classify(768, 7)\n",
        "\n",
        "    def wav_encoder(self, wav_dir):\n",
        "        # load audio\n",
        "        #audio_input, sample_rate = sf.read(wav_dir)\n",
        "        # pad input values and return pt tensor\n",
        "        #input_values = processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n",
        "        # retrieve logits & take argmax\n",
        "        logits = self.wav_model(wav_dir)['hidden_states'][-1]\n",
        "        # print(logits)\n",
        "        #predicted_ids = torch.argmax(logits, dim=-1)\n",
        "        return logits\n",
        "\n",
        "    def txt_encoder(self, text_tensor):\n",
        "\n",
        "        outputs = self.txt_model(**txt_tensor)\n",
        "        last_hidden_states = outputs.last_hidden_state\n",
        "        return last_hidden_states\n",
        "\n",
        "    def forward(self, text_data, wav_dir):\n",
        "        #encoder_layer_1 = wav_dir.squeeze(0)\n",
        "        encoder_layer_1 = self.wav_encoder(wav_dir)\n",
        "        encoder_layer_2 = self.txt_encoder(text_data)\n",
        "\n",
        "        out1 = self.project1(encoder_layer_1)\n",
        "        out2 = self.project2(encoder_layer_2)\n",
        "\n",
        "        # print(out1.size(), out2.size())\n",
        "        concat = torch.cat([out1, out2], dim = 1)\n",
        "        # (batch_size, 길이?, 768)\n",
        "        pool = nn.AdaptiveAvgPool2d((1,concat.size()[2]))\n",
        "        result = pool(concat).squeeze(dim=2)\n",
        "\n",
        "        logit = self.classification_model(result).squeeze(1)\n",
        "        softmax = F.softmax(logit, dim=1)\n",
        "        prediction = torch.argmax(softmax, dim=1)\n",
        "\n",
        "        return logit, softmax, prediction\n",
        "\n",
        "class Project(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "      super().__init__()\n",
        "      self.layer = nn.Linear(dim, dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "      return self.layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Cross-Attention"
      ],
      "metadata": {
        "id": "5J_hWtOM_dFt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzI3indYCc46"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_head):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_head = n_head\n",
        "        self.d_k = self.d_model // self.n_head\n",
        "\n",
        "        # Q, K, V에 대한 Linear Layers\n",
        "        self.w_qs = nn.Linear(d_model, n_head * self.d_k)\n",
        "        self.w_ks = nn.Linear(d_model, n_head * self.d_k)\n",
        "        self.w_vs = nn.Linear(d_model, n_head * self.d_k)\n",
        "\n",
        "        # Scaled Dot-Product Attention용 Linear Layer\n",
        "        self.fc = nn.Linear(n_head * self.d_k, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        # Q, K, V에 대한 Linear Layers를 지난 후, head를 나누는 것이 핵심\n",
        "        qs = self.w_qs(q).view(batch_size, -1, self.n_head, self.d_k)\n",
        "        ks = self.w_ks(k).view(batch_size, -1, self.n_head, self.d_k)\n",
        "        vs = self.w_vs(v).view(batch_size, -1, self.n_head, self.d_k)\n",
        "\n",
        "        # head를 transpose해서 batch_size와 head를 맞바꿔준다\n",
        "        qs = qs.transpose(1,2).contiguous().view(batch_size * self.n_head, -1, self.d_k)\n",
        "        ks = ks.transpose(1,2).contiguous().view(batch_size * self.n_head, -1, self.d_k)\n",
        "        vs = vs.transpose(1,2).contiguous().view(batch_size * self.n_head, -1, self.d_k)\n",
        "\n",
        "        # Scaled Dot-Product Attention\n",
        "        attn = torch.bmm(qs, ks.transpose(1, 2)) / (self.d_k ** 0.5)\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e9)\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        output = torch.bmm(attn, vs)\n",
        "\n",
        "        # Concatenate multi-heads\n",
        "        output = output.view(batch_size, self.n_head, -1, self.d_k)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.n_head * self.d_k)\n",
        "\n",
        "        # Linear Layer for output\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgTVrPJSGLgV"
      },
      "outputs": [],
      "source": [
        "# transformer cross_attention\n",
        "class Classify(nn.Module):\n",
        "  def __init__(self, input_size, class_num):\n",
        "    super(Classify, self).__init__()\n",
        "    self.fc = nn.Linear(input_size, class_num)\n",
        "\n",
        "  def forward(self, avg_vecs):\n",
        "    logit = self.fc(avg_vecs)\n",
        "    return logit\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.wav_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\", output_hidden_states=True)\n",
        "        self.txt_model = AutoModel.from_pretrained(\"beomi/KcELECTRA-base-v2022\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base-v2022\")\n",
        "        self.wav_encoder\n",
        "        self.txt_encoder\n",
        "        self.project1 = Project(768)\n",
        "        self.project2 = Project(768)\n",
        "        self.classification_model = Classify(768, 7)\n",
        "        self.attention = MultiHeadAttention(768, 8)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 7))\n",
        "      \n",
        "\n",
        "    def wav_encoder(self, wav_dir):\n",
        "        logits = self.wav_model(wav_dir)['hidden_states'][-1]\n",
        "        return logits\n",
        "\n",
        "    def txt_encoder(self, text_tensor):\n",
        "        outputs = self.txt_model(**txt_tensor)\n",
        "        last_hidden_states = outputs.last_hidden_state\n",
        "        return last_hidden_states\n",
        "\n",
        "    def forward(self, text_data, wav_dir):\n",
        "        encoder_layer_1 = self.wav_encoder(wav_dir)\n",
        "        encoder_layer_2 = self.txt_encoder(text_data)\n",
        "\n",
        "        out1 = self.project1(encoder_layer_1)\n",
        "        out2 = self.project2(encoder_layer_2)\n",
        "\n",
        "        concat = self.attention(out1, out2, out2)\n",
        "        pool = nn.AdaptiveAvgPool2d((1, concat.size()[2]))\n",
        "        result = pool(concat).squeeze(dim=1)\n",
        "\n",
        "        logit = self.classification_model(result)\n",
        "\n",
        "        softmax = F.softmax(logit, dim=1)\n",
        "        prediction = torch.argmax(softmax, dim=1)\n",
        "\n",
        "        return logit, softmax, prediction\n",
        "\n",
        "class Project(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "      super().__init__()\n",
        "      self.layer = nn.Linear(dim, dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "      return self.layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. 3-way"
      ],
      "metadata": {
        "id": "MVVdm3kt_sxY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpOXNp99TRTy"
      },
      "outputs": [],
      "source": [
        "# 3-way\n",
        "class Classify(nn.Module):\n",
        "  def __init__(self, input_size, class_num):\n",
        "    super(Classify, self).__init__()\n",
        "    self.fc = nn.Linear(input_size, class_num)\n",
        "\n",
        "  def forward(self, avg_vecs):\n",
        "    logit = self.fc(avg_vecs)\n",
        "    return logit\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.wav_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\", output_hidden_states=True)\n",
        "        self.txt_model = AutoModel.from_pretrained(\"beomi/KcELECTRA-base-v2022\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base-v2022\")\n",
        "        self.wav_encoder\n",
        "        self.txt_encoder\n",
        "        self.project1 = Project(768)\n",
        "        self.project2 = Project(768)\n",
        "        self.classification_model = Classify(768, 7)\n",
        "        self.attention = MultiHeadAttention(768, 8)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 7))\n",
        "      \n",
        "\n",
        "    def wav_encoder(self, wav_dir):\n",
        "        logits = self.wav_model(wav_dir)['hidden_states'][-1]\n",
        "        return logits\n",
        "\n",
        "    def txt_encoder(self, text_tensor):\n",
        "        outputs = self.txt_model(**txt_tensor)\n",
        "        last_hidden_states = outputs.last_hidden_state\n",
        "        return last_hidden_states\n",
        "\n",
        "    def forward(self, text_data, wav_dir):\n",
        "        encoder_layer_1 = self.wav_encoder(wav_dir)\n",
        "        encoder_layer_2 = self.txt_encoder(text_data)\n",
        "\n",
        "        out1 = self.project1(encoder_layer_1)\n",
        "        out2 = self.project2(encoder_layer_2)\n",
        "\n",
        "        concat = torch.cat([out1, out2], dim = 1)\n",
        "        min_length = min(len(out1[0]), len(out2[0]))\n",
        "\n",
        "        # 요소 곱\n",
        "        concat_1 = out1[:, :min_length, :] * out2[:, :min_length, :]\n",
        "\n",
        "        # 요소 차\n",
        "        concat_2 = torch.abs(out1[:, :min_length, :] - out2[:, :min_length, :])\n",
        "\n",
        "        # 3-way concat\n",
        "        concat_3 = torch.cat([concat, concat_1, concat_2], dim = 1)\n",
        "\n",
        "        pool = nn.AdaptiveAvgPool2d((1,concat.size()[2]))\n",
        "        result = pool(concat).squeeze(dim=1)\n",
        "\n",
        "        logit = self.classification_model(result)\n",
        "\n",
        "        softmax = F.softmax(logit, dim=1)\n",
        "        prediction = torch.argmax(softmax, dim=1)\n",
        "\n",
        "        return logit, softmax, prediction\n",
        "\n",
        "class Project(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "      super().__init__()\n",
        "      self.layer = nn.Linear(dim, dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "      return self.layer(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQb7bKS8Gnf4"
      },
      "outputs": [],
      "source": [
        "model = Model()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train & Valid"
      ],
      "metadata": {
        "id": "GzjCNMhg_w4W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgFS0dvTCZCI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68SXyRxu2Zhx"
      },
      "outputs": [],
      "source": [
        "model.txt_model.requires_grad, model.wav_model.requires_grad = False, False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0wq7bRiwn6j"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "train_dataset = CustomDataset('KEMDy20_train_data.csv')\n",
        "valid_dataset = CustomDataset('KEMDy20_val_data.csv')\n",
        "test_dataset = CustomDataset('KEMDy20_test_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yhx7kADzsrR"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers = 3)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=True, num_workers = 3)\n",
        "\n",
        "class_weights = [1/1274,1/180,1/197,1/9098,1/94,1/53,1/182]  \n",
        "criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights)).to(device)\n",
        "model.train()\n",
        "model.txt_model.requires_grad, model.wav_model.requires_grad = False, False\n",
        "optimizer = optim.SGD(filter(lambda x: x.requires_grad, model.parameters()), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JihrR6ngzuHb"
      },
      "outputs": [],
      "source": [
        "model = model.to('cuda')\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(t:=tqdm(train_loader)):\n",
        "        txt_data, input_values, labels = data\n",
        "\n",
        "        txt_tensor = model.tokenizer(txt_data, return_tensors=\"pt\", padding = True)\n",
        "        \n",
        "        for key in txt_tensor.keys():\n",
        "            txt_tensor[key] = txt_tensor[key].to(\"cuda\")\n",
        "        input_values = input_values.to(\"cuda\")\n",
        "        labels = labels.to(\"cuda\")\n",
        "\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        _, softmax, outputs = model(txt_tensor, input_values)\n",
        "        loss = criterion(softmax, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        t.set_postfix_str(f' Loss: {running_loss/(i+1)}')\n",
        "\n",
        "        \n",
        "    model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    pred_label = []\n",
        "    true_label = []\n",
        "\n",
        "    for i, data in enumerate(t:=tqdm(valid_loader)):\n",
        "        txt_data, input_values, labels = data\n",
        "\n",
        "        txt_tensor = model.tokenizer(txt_data, return_tensors=\"pt\", padding = True)\n",
        "        \n",
        "        for key in txt_tensor.keys():\n",
        "            txt_tensor[key] = txt_tensor[key].to(\"cuda\")\n",
        "        input_values = input_values.to(\"cuda\")\n",
        "        labels = labels.to(\"cuda\")\n",
        "        \n",
        "        _, softmax, outputs = model(txt_tensor, input_values)\n",
        "        loss = criterion(softmax, labels)\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        outputs = outputs.detach().to('cpu')\n",
        "        labels = labels.detach().to('cpu')\n",
        "        pred_label += outputs.tolist()\n",
        "        true_label += labels.tolist()\n",
        "\n",
        "        t.set_postfix_str(f' Val_Loss: {running_loss/(i+1)}')\n",
        "\n",
        "    model.train()\n",
        "    f1score = f1_score(true_label, pred_label, average='weighted')\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}, F1_score: {f1score}')\n",
        "    print(classification_report(true_label, pred_label))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPU 메모리 초기화"
      ],
      "metadata": {
        "id": "VLgI20fa_5og"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WZ-JxmXuwrG",
        "outputId": "964d5172-6c4c-45f1-c649-f9c939a76af8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14600372224"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.memory_allocated()\n",
        "torch.cuda.memory_reserved()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RDfuDR3tT41"
      },
      "outputs": [],
      "source": [
        "import torch as th\n",
        "del input_values\n",
        "del labels\n",
        "for i in list(txt_tensor.keys()):\n",
        "  del txt_tensor[i]\n",
        "del outputs\n",
        "del loss\n",
        "del model\n",
        "\n",
        "th.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}