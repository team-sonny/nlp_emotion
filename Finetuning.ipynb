{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "864YRk7kn4qc"
   },
   "source": [
    "## module import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sXMvGrZen0We",
    "outputId": "b22ebd04-d9e1-417e-dad9-eca4b8ca8399",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-09 07:35:21.029123: I tensorflow/core/tpu/tpu_api_dlsym_initializer.cc:116] Libtpu path is: libtpu.so\n",
      "[percpu.cc : 539] RAW: rseq syscall failed with errno 22 after membarrier sycall succeeded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.10.0+cu102\n",
      "tf version: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric, ClassLabel, Sequence, Dataset\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback, AutoModelForMaskedLM\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('torch version:', torch.__version__)\n",
    "print('tf version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10\n"
     ]
    }
   ],
   "source": [
    "# using TPU through torch\n",
    "import torch_xla\n",
    "import torch_xla.utils.utils as xu\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.utils.serialization as xser\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "\n",
    "print(torch_xla.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7Pk4EHCMYWs7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# random seed fix\n",
    "import random\n",
    "\n",
    "random.seed(2022)\n",
    "torch.manual_seed(2022)\n",
    "np.random.seed(2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPU setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-09 07:35:24.475751: E tensorflow/core/framework/op_kernel.cc:1676] OpKernel ('op: \"TPURoundRobin\" device_type: \"CPU\"') for unknown op: TPURoundRobin\n",
      "2023-04-09 07:35:24.475804: E tensorflow/core/framework/op_kernel.cc:1676] OpKernel ('op: \"TpuHandleToProtoKey\" device_type: \"CPU\"') for unknown op: TpuHandleToProtoKey\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='xla', index=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Google cloud project에서 TPU 셋팅\n",
    "\n",
    "# .py로 실행할 때 TPU 셋팅 명령어\n",
    "#!export XRT_TPU_CONFIG=\"localservice;0;localhost:51011\"\n",
    "\n",
    "# 주피터 노트 또는 주피터 랩에서 실행할 때, TPU 셋팅 명령어\n",
    "import os\n",
    "os.environ['XRT_TPU_CONFIG'] = \"localservice;0;localhost:51011\"\n",
    "device = xm.xla_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LObHhscW1CH"
   },
   "source": [
    "## data load & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yejin/sonny\n"
     ]
    }
   ],
   "source": [
    "cd ~/sonny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/yejin/sonny'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;36m'015.감성 및 발화 스타일별 음성합성 데이터'\u001b[0m@\n",
      " \u001b[01;36mKEMDy19\u001b[0m@\n",
      " \u001b[01;36mKEMDy19.7z\u001b[0m@\n",
      " \u001b[01;36mKEMDy20\u001b[0m@\n",
      " \u001b[01;36mKEMDy20.7z\u001b[0m@\n",
      " \u001b[01;36mKEMDy20_test_data.csv\u001b[0m@\n",
      " \u001b[01;36mKEMDy20_train_data.csv\u001b[0m@\n",
      " \u001b[01;36mKEMDy20_val_data.csv\u001b[0m@\n",
      " \u001b[01;34mRoBERTa_batch128dr\u001b[0m/\n",
      " \u001b[01;34mRoBERTa_batch8\u001b[0m/\n",
      " \u001b[01;34mRoBERTa_batch8dr\u001b[0m/\n",
      " \u001b[01;36mbalance_train.csv\u001b[0m@\n",
      " \u001b[01;36mdataset\u001b[0m@\n",
      " \u001b[01;36mdataset-20230325T131448Z-001.zip\u001b[0m@\n",
      " \u001b[01;36mdataset-20230325T131448Z-002.zip\u001b[0m@\n",
      " \u001b[01;36memotion_aihub\u001b[0m@\n",
      " \u001b[01;36mlost+found\u001b[0m@\n",
      " \u001b[01;34mnlp_emotion\u001b[0m/\n",
      " \u001b[01;34mutils\u001b[0m/\n",
      " \u001b[01;34mwandb\u001b[0m/\n",
      " \u001b[01;36mwav_data\u001b[0m@\n",
      " \u001b[01;36mwav_data.7z\u001b[0m@\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbMXQ5oHYrml"
   },
   "source": [
    "## input data transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316,
     "referenced_widgets": [
      "34e1eb37ce764d13b878be6a530bd27c",
      "604164fb70af4e1b949e88922d4c4e3a",
      "e4510b9917ce4cf0abd6190fe3303c5f",
      "424d7d8450e94bf080e22ca965ce7c43",
      "e89269f7a35a46489899d3c9f0eb5bd6",
      "89e1594e6cda4b1db8031451f91b84f5",
      "e786c5400cdc4378aae78edaa2de39cf",
      "1c7dd6d47eac4b7194d8f519a4f4b6de",
      "bdcc026635ba4e4ea611dad619e61c5c",
      "2d33612f58904d52985783299ff1d985",
      "b564354eb5074a7e9c15d175a2aee4e9",
      "dd0839d0c3314b58a373f0e81258a45a",
      "8cedf6f45e19454b88667900e1b427a4",
      "4aec821693ce4d5f95c65604854fc5d7",
      "ba21c9755fb84f25a5bb2d644e334b6d",
      "c53687566b444ceb982d074f368b3eec",
      "23678fa78d2e4b8abd6dd0389102b33b",
      "d0cb2262e4be4ef4bb4c46ac198ff65e",
      "e8ff0606b2d14c7e9d830a795c1af1aa",
      "72c43724d4f840b68b21cf5a4ff998c0",
      "db645793f2cc40e6b9b13f09851ec989",
      "20f4a5bb0e6e477e9d6d4a8590b2c309",
      "000f4268403b403186a54f32ae4dba03",
      "d3279340af2141209c8b34055f655355",
      "82ec93d9922b4c24aea277d809fbdec0",
      "73f0104175ee4e229a59cad01f1ad6ce",
      "2f632506c4734b789044236ad8513741",
      "d83cf60951f54eec87ee219240b0269c",
      "d8e31f281c584c26b01204d198b2eaf7",
      "620afb6f79ee4ee7ae074d6ca57d637d",
      "dfa8963d4873443280953a60360c6c5d",
      "dc9d254ca88840068d6d838d1ebd9c6d",
      "25cab45546b94d28b81955feb8fa54b3",
      "71a481c6b92e4759b67c419dd92ef828",
      "d6fc4a71e71d460a8491c285cdc82916",
      "23ea92737a6d405c9c4737a416b11c70",
      "146185d69bac482bb8196799358f87a0",
      "1f7e851e0429451e8e9c0442eabd3115",
      "2cf6c5def959431f8fede3fc13f03bcf",
      "08d66693e6f346e380eb9d4a2f47a10a",
      "6601ea2d165947d4a0da0402e996b100",
      "5cdd9c2b19ef4258b14d7b2eee6da0a2",
      "90ce0cda6a934fca850412f292911262",
      "74756bb09457468b9c88a66e8f419a6b",
      "5924ceda30a2473b855ff3531975f6ab",
      "a1093d08f29a43acb0f79c38d19149b7",
      "f500442463684e688eb2ca534e5d238b",
      "f605ef8fc931466793bd15ecc2a182d2",
      "e51b6c26c6624b0f8acfef67d03a3254",
      "f468cc901a34493ba8ef2c5f7caba4aa",
      "439cd5c66f12497d9ee72c655db317d3",
      "69e8731b94a0450c92e8ab7534339db2",
      "8624b8be17ed47a3bb83de7b04ac684c",
      "000a64becdb842efb596d7d4184e1691",
      "1ae91369308245bcaf746da96625fa6f",
      "1dd0f195ea4246568f42e16fbf32dbba",
      "d377d9c88701402f80a2d324be3873e0",
      "b7e07ff654d148128accc6e6346999d6",
      "797af7c14b4043449498c76ed3439f38",
      "62176805172e4e5384fcbeaf7e8dbb54",
      "f1d451598c42493a93f827dbaf949fd5",
      "a67d54bda75d407c97eb40e17fc7d5a8",
      "2100cdb2bbc44a989b6b60ab6470c84d",
      "14c6e6ac869343f9be95f84ce408f76a",
      "b0a1cf67e5b94b9da661524dcfe8575d",
      "bd03e8adb54e45528040b4c96803c25c"
     ]
    },
    "id": "c4au8-gmZAt8",
    "outputId": "85da58b2-6d5b-4320-ff67-2242b691e01d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load DAPT model, tokenizer\n",
    "num_labels = 7\n",
    "model = AutoModelForSequenceClassification.from_pretrained('klue/roberta-large', num_labels=num_labels)\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yejin/sonny/utils/Customdataset.py:43: DtypeWarning: Columns (3,7,10,13,16,19,22,25,28,31,34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.data = pd.read_csv(csv_path, header=[0, 1])\n"
     ]
    }
   ],
   "source": [
    "from utils.Customdataset import CustomDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "train_dataset = CustomDataset('balance_train.csv')\n",
    "val_dataset = CustomDataset('KEMDy20_val_data.csv')\n",
    "test_dataset = CustomDataset('KEMDy20_test_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('신세 진 것은 꼭 갚아야겠다고 했다.', 0),\n",
       " ('뒤이어 흐느껴 우는 소리가 들렸습니다.', 6),\n",
       " ('그에게 불쌍한 그녀가 계속 보였다.', 6))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1],train_dataset[2],train_dataset[0],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nsqQqGwZYmv2",
    "outputId": "5652a240-b485-48d6-eb54-52255cb9cccb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70018\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ml3V_dmAd2gL",
    "outputId": "7d5d648b-88e1-4e22-c5f4-8cb7e45d4215",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers = 3, collate_fn=preprocess_function)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=2, shuffle=True, num_workers = 3, collate_fn=preprocess_function)\n",
    "\n",
    "# class_weights = [1/1274,1/180,1/197,1/9098,1/94,1/53,1/182]\n",
    "# criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPJUwiF5qnad"
   },
   "source": [
    "## eval metric 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5FQTgTfpuUCH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "multi sentiment analysis는 기본적으로 문장 분류 문제\n",
    "KLUE task를 학습한 klue-roberta 중에서 TC(topic classification)와 유사(완전히 같지는 않음)\n",
    "TC는 평가 지표로 macro f1 score를 사용\n",
    "\n",
    "but klue에서는 다중감성분류를 학습하지 않았고,\n",
    "glue에서는 sst2라는 긍부정 분류 task가 있는데 accuracy를 사용\n",
    "\n",
    "but 우리는 데이터셋이 다소 imbalance하기 때문에 accuracy를 채택하면 오차가 발생할 수 있음\n",
    "\n",
    "따라서 더 data imbalance에 robust한 metric를 만들기 위해\n",
    "sklearn에서 제공하는 f1 score metric을 활용\n",
    "'''\n",
    "\n",
    "def eval_metric(pred, real):\n",
    "    f1 = {\n",
    "        'macro_f1':f1_score(real, pred, average='macro'),\n",
    "        'micro_f1':f1_score(real, pred, average='micro'),\n",
    "        'weighted_f1':f1_score(real, pred, average='weighted'), \n",
    "    }\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_AYObCxOx_BQ",
    "outputId": "c3f53dd6-65f4-4032-8a2c-bba3dcbcd958",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5, 4, 5, 0, 1, 1, 0, 0, 2, 0, 0, 5, 1, 1, 3, 3, 3, 0, 3, 0, 3, 0,\n",
       "        5, 0, 2, 2, 3, 1, 2, 0, 5, 5, 1, 5, 5, 4, 0, 2, 2, 2, 3, 4, 5, 1,\n",
       "        2, 4, 0, 5, 5, 4, 2, 5, 0, 5, 5, 5, 5, 5, 0, 3, 3, 2, 4, 0]),\n",
       " array([1, 4, 1, 0, 5, 5, 0, 0, 3, 1, 5, 3, 2, 1, 5, 2, 1, 4, 1, 2, 2, 1,\n",
       "        5, 1, 2, 1, 4, 3, 0, 4, 4, 4, 4, 0, 1, 2, 2, 4, 1, 5, 5, 2, 0, 0,\n",
       "        4, 0, 0, 3, 4, 3, 1, 2, 3, 2, 3, 2, 3, 4, 5, 3, 3, 5, 3, 4]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metric test\n",
    "np.random.seed(2022)\n",
    "fake_preds = np.random.randint(0, 6, size=(64,))\n",
    "fake_labels = np.random.randint(0, 6, size=(64,))\n",
    "fake_preds, fake_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cLY-NzOWy52K",
    "outputId": "f9318350-8fea-4790-8509-5074852d389a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_f1': 0.15364479575005888,\n",
       " 'micro_f1': 0.15625,\n",
       " 'weighted_f1': 0.1490065307499518}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metric(fake_preds, fake_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "9-75NjFHqTwc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eval metric\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    result = eval_metric(predictions, labels)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model to tpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezhr0RV_6_3z"
   },
   "source": [
    "## train arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "idjRcOQCcoNd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preprocessing function\n",
    "def preprocess_function(data):\n",
    "    data_T = zip(*data).__iter__()\n",
    "    out = {**tokenizer(\n",
    "        # tokenizing\n",
    "        next(data_T),\n",
    "        max_length=512,\n",
    "\n",
    "        # 최대 길이보다 긴 시퀀스는 최대 길이에 맞춰 자름\n",
    "        truncation=True,\n",
    "\n",
    "        # tokenizer가 token_type_ids를 return하지 않게 함\n",
    "        # roberta는 필요없기 때문\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    ), \"labels\":torch.tensor(next(data_T))\n",
    "    }\n",
    "    # for i in list(out):\n",
    "    #     out[i]=out[i].to(device)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NOTEBOOK_NAME=/home/yejin/sonny/nlp_emotion/Finetuning.ipynb\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_NOTEBOOK_NAME = /home/yejin/sonny/nlp_emotion/Finetuning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4KIxQY5t4lW",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=our_emotion\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/yejin/sonny/wandb/run-20230408_234801-iowqg8at</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smart-sprout/our_emotion/runs/iowqg8at' target=\"_blank\">RoBERTa_emotion_batch32</a></strong> to <a href='https://wandb.ai/smart-sprout/our_emotion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smart-sprout/our_emotion' target=\"_blank\">https://wandb.ai/smart-sprout/our_emotion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smart-sprout/our_emotion/runs/iowqg8at' target=\"_blank\">https://wandb.ai/smart-sprout/our_emotion/runs/iowqg8at</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='481' max='262575' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   481/262575 1:04:01 < 583:52:39, 0.12 it/s, Epoch 0.03/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.967900</td>\n",
       "      <td>2.022730</td>\n",
       "      <td>0.007063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.932800</td>\n",
       "      <td>1.724668</td>\n",
       "      <td>0.119862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# metric_name은 eval_metric에서 리턴받는 dict 형태의 키 이름\n",
    "metric_name = \"micro_f1\"\n",
    "\n",
    "# batch size 지정\n",
    "batch_size = 32\n",
    "num_train_epochs = 15\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Project name and run name\n",
    "run_name = f\"RoBERTa_emotion_batch{batch_size}\"\n",
    "%env WANDB_PROJECT = our_emotion\n",
    "\n",
    "# path 설정\n",
    "trained_model_path = f\"ro{batch_size}\"\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=trained_model_path,\n",
    "    overwrite_output_dir=True,\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=1e-6,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    save_total_limit=2,\n",
    "    tpu_num_cores = 85,\n",
    "    seed = 2023,\n",
    "    data_seed = 2023,\n",
    "    dataloader_pin_memory = True,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=run_name,\n",
    "    optim = 'adafactor',\n",
    "    logging_steps=10,\n",
    "    eval_steps=200,\n",
    "    save_steps=200\n",
    ")\n",
    " \n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    # tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=15)],\n",
    "    data_collator=preprocess_function\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"time :\", time.time() - start)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "pdnd3LYKPsm2"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vazz0901/.local/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:38: FutureWarning: Deprecated positional argument(s) used in 'create_repo': pass token='FT_lcs_adafactor_lr1e_6' as keyword args. From version 0.12 passing these as positional arguments will result in an error,\n",
      "  warnings.warn(\n",
      "/home/vazz0901/.local/lib/python3.8/site-packages/huggingface_hub/hf_api.py:681: FutureWarning: `create_repo` now takes `token` as an optional positional argument. Be sure to adapt your code!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/JUNEYEOB/FT_lcs_adafactor_lr1e_6 into local empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /tmp/tmpjnokmc9g/config.json\n",
      "tcmalloc: large alloc 1469087744 bytes == 0x1d10084000 @  0x7f59cc76c680 0x7f59cc78cda2 0x5f8dfc 0x64f870 0x527012 0x5c64c0 0x5f4cc1 0x5f4f85 0x486664 0x539ccb 0x539bf9 0x66321b 0x53a821 0x53a01f 0x6632cc 0x53a164 0x53a01f 0x66321b 0x53a164 0x53a8d8 0x66134d 0x6615f0 0x505166 0x56bbfa 0x569dba 0x5f6eb3 0x56bacd 0x569dba 0x5f6eb3 0x56bacd 0x569dba\n",
      "tcmalloc: large alloc 1836359680 bytes == 0x1c6c32c000 @  0x7f59cc76c680 0x7f59cc78cda2 0x5f8dfc 0x64f870 0x527012 0x5c64c0 0x5f4cc1 0x5f4f85 0x486664 0x539ccb 0x539bf9 0x66321b 0x53a821 0x53a01f 0x6632cc 0x53a164 0x53a01f 0x66321b 0x53a164 0x53a8d8 0x66134d 0x6615f0 0x505166 0x56bbfa 0x569dba 0x5f6eb3 0x56bacd 0x569dba 0x5f6eb3 0x56bacd 0x569dba\n",
      "tcmalloc: large alloc 2295455744 bytes == 0x1d52c28000 @  0x7f59cc76c680 0x7f59cc78cda2 0x5f8dfc 0x64f870 0x527012 0x5c64c0 0x5f4cc1 0x5f4f85 0x486664 0x539ccb 0x539bf9 0x66321b 0x53a821 0x53a01f 0x6632cc 0x53a164 0x53a01f 0x66321b 0x53a164 0x53a8d8 0x66134d 0x6615f0 0x505166 0x56bbfa 0x569dba 0x5f6eb3 0x56bacd 0x569dba 0x5f6eb3 0x56bacd 0x569dba\n",
      "tcmalloc: large alloc 2015535104 bytes == 0x1df051a000 @  0x7f59cc76c680 0x7f59cc78cff4 0x7f59cc281379 0x7f59cc283009 0x7f59cc2830a6 0x7f584d5fb5b0 0x7f584d6c4581 0x7f584d35c8d5 0x5f6929 0x5f74f6 0x50c383 0x570b26 0x569dba 0x5f6eb3 0x56bacd 0x569dba 0x5f6eb3 0x56bacd 0x569dba 0x50bca0 0x56cc1f 0x569dba 0x50bca0 0x56cc1f 0x569dba 0x6902a7 0x6023c4 0x5c6730 0x56bacd 0x501488 0x56d4d6\n",
      "Model weights saved in /tmp/tmpjnokmc9g/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5a6ce40666421598a934dea66e4adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/JUNEYEOB/FT_lcs_adafactor_lr1e_6\n",
      "   86e9f45..fb805fc  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/JUNEYEOB/FT_lcs_adafactor_lr1e_6/commit/fb805fcd88543d8e9b455a137be7fbd0867e249f'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model, tokenizer\n",
    "# batch_size = 32 \n",
    "# model = AutoModelForSequenceClassification.from_pretrained('jungyong/FT_batch32_lyric')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('klue/roberta-large')\n",
    "\n",
    "MODEL_SAVE_REPO = f'FT_lcs_{run_name}'\n",
    "HUGGINGFACE_AUTO_TOKEN = input() # https://huggingface.co/settings/token\n",
    " \n",
    "## Push to huggingface-hub\n",
    "model.push_to_hub(\n",
    "    MODEL_SAVE_REPO, \n",
    "    use_temp_dir=True, \n",
    "    use_auth_token=HUGGINGFACE_AUTO_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'FT_lcs_adafactor_lr1e_6'...\n",
      "remote: Enumerating objects: 7, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
      "remote: Total 7 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (7/7), 1.29 KiB | 1.29 MiB/s, done.\n",
      "tcmalloc: large alloc 1471086592 bytes == 0x560e8acde000 @  0x7f12d2724680 0x7f12d2744bdd 0x560e1c0f06b5 0x560e1c0bd840 0x560e1c05b6a1 0x560e1bfe496a 0x560e1bfe51a2 0x560e1bfe5bf2 0x560e1c009f3c 0x560e1c00a4e4 0x560e1c00ab32 0x560e1c0e4eac 0x560e1bf13f2c 0x560e1bef42f4 0x560e1bef53b4 0x560e1bef3e9e 0x7f12d2468083 0x560e1bef3f0e\n",
      "tcmalloc: large alloc 2206621696 bytes == 0x560ee27ce000 @  0x7f12d2724680 0x7f12d2744bdd 0x560e1c0f06b5 0x560e1c0bd840 0x560e1c05b6a1 0x560e1bfe496a 0x560e1bfe51a2 0x560e1bfe5bf2 0x560e1c009f3c 0x560e1c00a4e4 0x560e1c00ab32 0x560e1c0e4eac 0x560e1bf13f2c 0x560e1bef42f4 0x560e1bef53b4 0x560e1bef3e9e 0x7f12d2468083 0x560e1bef3f0e\n"
     ]
    }
   ],
   "source": [
    "!git clone https://huggingface.co/JUNEYEOB/FT_lcs_adafactor_lr1e_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd FT_lcs_adafactor_lr1e_6\n",
    "!cp -r ~/batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
